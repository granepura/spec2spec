{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed17f6a-3a9c-42a3-88fc-395e9b8b393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import astropy.io.fits as fits\n",
    "from astropy.table import Table\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fa46a54-2483-4d08-a343-f48c7a118dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "import time\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a48b556c-c1f8-4141-85e5-2b86b53976ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59266, 83) Index(['DRPVER', 'MPROCVER', 'MANGAID', 'PLATE', 'IFUDESIGN', 'MJD', 'RA',\n",
      "       'DEC', 'EPOCH', 'IFURA', 'IFUDEC', 'MNGTARG2', 'MJDQUAL', 'TEFF_MED',\n",
      "       'LOGG_MED', 'FEH_MED', 'FEH_CAL_MED', 'ALPHA_MED', 'ZH_MED',\n",
      "       'ZH_CAL_MED', 'TEFF_MED_ERR', 'LOGG_MED_ERR', 'FEH_MED_ERR',\n",
      "       'FEH_CAL_MED_ERR', 'ALPHA_MED_ERR', 'ZH_MED_ERR', 'ZH_CAL_MED_ERR',\n",
      "       'NGROUPS', 'INPUT_GROUPS_NAME', 'NALPHAGROUPS',\n",
      "       'INPUT_ALPHA_GROUPS_NAME', 'TEFF_JI', 'LOGG_JI', 'FEH_JI', 'ALPHA_JI',\n",
      "       'VMICRO_JI', 'TEFF_ERR_JI', 'LOGG_ERR_JI', 'FEH_ERR_JI', 'ALPHA_ERR_JI',\n",
      "       'VMICRO_ERR_JI', 'CHISQ_JI', 'VALID_JI', 'TEFF_DL', 'LOGG_DL', 'FEH_DL',\n",
      "       'ALPHA_DL', 'TEFF_ERR_DL', 'LOGG_ERR_DL', 'FEH_ERR_DL', 'ALPHA_ERR_DL',\n",
      "       'CHISQ_DL', 'AV_DL', 'AV_ERR_DL', 'AV_CHISQ_DL', 'AV_VALID_DL',\n",
      "       'VALID_DL', 'TEFF_LH', 'LOGG_LH', 'FEH_LH', 'ALPHA_LH',\n",
      "       'TEFF_ERR_UP_LH', 'TEFF_ERR_DN_LH', 'LOGG_ERR_UP_LH', 'LOGG_ERR_DN_LH',\n",
      "       'FEH_ERR_UP_LH', 'FEH_ERR_DN_LH', 'ALPHA_ERR_UP_LH', 'ALPHA_ERR_DN_LH',\n",
      "       'CHISQ_LH', 'MODEL_LH', 'VALID_LH', 'TEFF_YC', 'LOGG_YC', 'FEH_YC',\n",
      "       'ALPHA_YC', 'TEFF_ERR_YC', 'LOGG_ERR_YC', 'FEH_ERR_YC', 'ALPHA_ERR_YC',\n",
      "       'CHISQ_YC', 'MODEL_YC', 'VALID_YC'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/data/jdli/mastar/\"\n",
    "mastarall = fits.open(data_dir + 'mastarall-v3_1_1-v1_7_7.fits')\n",
    "\n",
    "# params = fits.open(data_dir + 'mastar-goodvisits-v3_1_1-v1_7_7-params-v1.fits')[1].data\n",
    "tbl = Table.read(data_dir + 'mastar-goodvisits-v3_1_1-v1_7_7-params-v1.fits')\n",
    "names = [name for name in tbl.colnames if len(tbl[name].shape) <= 1]\n",
    "df = tbl[names].to_pandas()\n",
    "\n",
    "print(df.shape, df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdff66c3-3e41-4c9e-84ac-23c2a7847945",
   "metadata": {},
   "outputs": [],
   "source": [
    "photom = fits.open(data_dir + 'mastarall-gaiaedr3-extcorr-simbad-ps1-v3_1_1-v1_7_7-v1.fits')[1].data\n",
    "bprp_gaia, g_mag = photom['BPRPC'], photom['M_G']   #photometry\n",
    "clean_match = photom['GAIA_CLEANMATCH']          #mask of those entries with a clean match in Gaia\n",
    "mask = np.where((bprp_gaia > -999)&(clean_match == 1))\n",
    "\n",
    "goodspec = fits.open(data_dir+'mastar-goodspec-v3_1_1-v1_7_7.fits.gz')[1].data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cefddb3-6afe-4ded-9ae6-10a20d74fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(bprp_gaia[mask], g_mag[mask], 'k.')\n",
    "# plt.gca().invert_yaxis()\n",
    "# plt.xlabel('G_BP-G_RP')\n",
    "# plt.ylabel('M_G')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00fa9f22-760e-44eb-8139-f5619c97f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visits = (goodspec[\"MANGAID\"] == photom[0][\"MANGAID\"])\n",
    "\n",
    "# wl = goodspec[visits][\"WAVE\"][0]\n",
    "# flux = goodspec[visits][\"FLUX\"][0]\n",
    "\n",
    "# plt.plot(wl,flux)\n",
    "# plt.ylabel(\"flux (1e-17 erg/s/cm$^2$/Angstrom\")\n",
    "# plt.xlabel(\"wavelength (Angstroms)\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ee9ba94-8aaa-446d-b345-83cebf38b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "class Mastar():\n",
    "    \"\"\" \n",
    "    MaStar spectra instance\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pars, specs, device=torch.device('cuda:0')):\n",
    "        self.waves, self.fluxes = specs[\"WAVE\"], specs[\"FLUX\"]\n",
    "        self.device = device\n",
    "        self.MANGAID  = np.array(pars[\"MANGAID\"])\n",
    "        self.MANGAID_specs = np.array(goodspec[\"MANGAID\"])\n",
    "        self.pars = np.c_[pars['BPRPC'], pars['M_G']]\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        num_sets = len(self.MANGAID)\n",
    "        return num_sets\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        mangaid = self.MANGAID[idx]\n",
    "        visit   = self.MANGAID_specs==mangaid\n",
    "        flux    = torch.tensor(self.fluxes[visit][0].reshape(-1,1).astype(np.float32))\n",
    "        output  = torch.tensor(self.pars[idx].reshape(-1,1).astype(np.float32))\n",
    "        \n",
    "        return flux.to(self.device), output.to(self.device)\n",
    "    \n",
    "    \n",
    "mastar = Mastar(photom[mask][:5000], goodspec, device=torch.device('cuda:1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de05d1f6-5a24-4c9c-81d2-76f58fdf0bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "(tensor([[ 54.3723],\n",
      "        [ 97.2568],\n",
      "        [105.0218],\n",
      "        ...,\n",
      "        [ 39.6544],\n",
      "        [ 38.2300],\n",
      "        [ 45.0859]], device='cuda:1'), tensor([[0.7034],\n",
      "        [5.2200]], device='cuda:1'))\n"
     ]
    }
   ],
   "source": [
    "print(len(mastar))\n",
    "print(mastar[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d54f83b-a330-49c3-9a10-d26e55b4cec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4563, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mastar[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "068bfc83-2f7b-4335-a034-193fc85779eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self,  dropout: float=0.1, max_seq_len:int=5000, d_model:int=512, batch_first:bool=False\n",
    "        ):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            dropout: the dropout rate\n",
    "            max_seq_len: the maximum length of the input sequences\n",
    "            d_model: The dimension of the output of sub-layers in the model \n",
    "                     (Vaswani et al, 2017)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.batch_first = batch_first\n",
    "        self.x_dim = 1 if batch_first else 0\n",
    "\n",
    "        # copy pasted from PyTorch tutorial\n",
    "        position = torch.arange(max_seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_seq_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, enc_seq_len, dim_val] or \n",
    "               [enc_seq_len, batch_size, dim_val]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(self.x_dim)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "# import copy\n",
    "\n",
    "# def _get_clones(nn.module, N):\n",
    "#     return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "\n",
    "# class TransformerEncoder(nn.Module):\n",
    "#     __constants__ = ['norm']\n",
    "#     def __init__(self, encoder_layer, num_layers, norm=None):\n",
    "#         super(TransformerEncoder, self).__init__()\n",
    "#         self.layers = _get_clones(encoder_layer, num_layers)\n",
    "#         self.num_layers = num_layers\n",
    "#         self.norm = norm\n",
    "\n",
    "#     def forward(self, src, mask=None, src_key_padding_mask=None):\n",
    "#         # type: (Tensor, Optional[Tensor], Optional[Tensor]) -> Tensor\n",
    "#         output = src\n",
    "#         weights = []\n",
    "#         for mod in self.layers:\n",
    "#             output, weight = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
    "#             weights.append(weight)\n",
    "#         if self.norm is not None:\n",
    "#             output = self.norm(output)\n",
    "#         return output, weights  \n",
    "\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    A detailed description of the code can be found in my article here:\n",
    "    https://towardsdatascience.com/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e\n",
    "    [1] Wu, N., Green, B., Ben, X., O'banion, S. (2020). \n",
    "    [2] Vaswani, A. et al. (2017) \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size:int, dec_seq_len:int, batch_first:bool, out_seq_len:int=58,\n",
    "                 dim_val:int=512, n_encoder_layers:int=4, n_decoder_layers:int=4, n_heads:int=8,\n",
    "                 dropout_encoder: float=0.2, dropout_decoder: float=0.2, dropout_pos_enc: float=0.1,\n",
    "                 dim_feedforward_encoder: int=2048, dim_feedforward_decoder: int=2048, num_predicted_features: int=1\n",
    "                ): \n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "\n",
    "            input_size: int, number of input variables. 1 if univariate.\n",
    "            dec_seq_len: int, the length of the input sequence fed to the decoder\n",
    "            dim_val: int, aka d_model. All sub-layers in the model produce outputs of dimension dim_val\n",
    "            n_encoder_layers: int, number of stacked encoder layers in the encoder\n",
    "            n_decoder_layers: int, number of stacked encoder layers in the decoder\n",
    "            n_heads: int, the number of attention heads (aka parallel attention layers)\n",
    "\n",
    "            dropout_encoder: float, the dropout rate of the encoder\n",
    "            dropout_decoder: float, the dropout rate of the decoder\n",
    "            dropout_pos_enc: float, the dropout rate of the positional encoder\n",
    "\n",
    "            dim_feedforward_encoder: int, number of neurons in the linear layer of the encoder\n",
    "            dim_feedforward_decoder: int, number of neurons in the linear layer of the decoder\n",
    "            num_predicted_features: int, the number of features you want to predict. Most of the time, this will be 1.\n",
    "        \"\"\"\n",
    "        super().__init__() \n",
    "\n",
    "        self.dec_seq_len = dec_seq_len\n",
    "\n",
    "        # Creating the three linear layers needed for the model\n",
    "        self.encoder_input_layer = nn.Linear(in_features=input_size, out_features=dim_val)\n",
    "        self.decoder_input_layer = nn.Linear(in_features=num_predicted_features, out_features=dim_val)\n",
    "        self.linear_mapping = nn.Linear(in_features=dim_val, out_features=num_predicted_features)\n",
    "\n",
    "        # Create positional encoder\n",
    "        self.positional_encoding_layer = PositionalEncoder(d_model=dim_val, dropout=dropout_pos_enc)\n",
    "\n",
    "        # The encoder layer used in the paper is identical to the one used by\n",
    "        # Vaswani et al (2017) on which the PyTorch module is based.\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim_val, nhead=n_heads, \n",
    "                                                   dim_feedforward=dim_feedforward_encoder, \n",
    "                                                   dropout=dropout_encoder, batch_first=batch_first)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer=encoder_layer, num_layers=n_encoder_layers, \n",
    "                                             norm=None)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=dim_val, nhead=n_heads,\n",
    "                                                   dim_feedforward=dim_feedforward_decoder,\n",
    "                                                   dropout=dropout_decoder, batch_first=batch_first)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer=decoder_layer, num_layers=n_decoder_layers, \n",
    "                                             norm=None)\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Tensor=None, \n",
    "                tgt_mask: Tensor=None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Returns a tensor of shape:\n",
    "        [target_sequence_length, batch_size, num_predicted_features]\n",
    "        \"\"\"\n",
    "        src = self.encoder_input_layer(src) \n",
    "        src = self.positional_encoding_layer(src)\n",
    "        src, weight = self.encoder(src) # src shape: [batch_size, enc_seq_len, dim_val]\n",
    "        decoder_output = self.decoder_input_layer(tgt) # src shape: [target sequence length, batch_size, dim_val] regardless of number of input features\n",
    "        decoder_output = self.decoder(tgt=decoder_output, memory=src,\n",
    "                                      tgt_mask=tgt_mask, memory_mask=src_mask)\n",
    "        # Pass through linear mapping\n",
    "        decoder_output = self.linear_mapping(decoder_output) # shape [batch_size, target seq len]\n",
    "        return decoder_output, weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e6d038-b55c-4068-ae99-1b15e6fc9518",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64ffdb72-d5a8-4985-acf9-ab0ef89dcb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model parameters\n",
    "dim_val = 512 # This can be any value divisible by n_heads. 512 is used in the original transformer paper.\n",
    "n_heads = 8 # The number of attention heads (aka parallel attention layers). dim_val must be divisible by this number\n",
    "n_decoder_layers = 4 # Number of times the decoder layer is stacked in the decoder\n",
    "n_encoder_layers = 4 # Number of times the encoder layer is stacked in the encoder\n",
    "input_size = 1 # The number of input variables. 1 if univariate forecasting.\n",
    "dec_seq_len = 4563 # length of input given to decoder. Can have any integer value.\n",
    "enc_seq_len = 4563 # length of input given to encoder. Can have any integer value.\n",
    "output_sequence_length = 2 # Length of the target sequence, i.e. how many time steps should your forecast cover\n",
    "# max_seq_len = 5000 # What's the longest sequence the model will encounter? Used to make the positional encoder\n",
    "\n",
    "device = torch.device('cuda:1')\n",
    "\n",
    "model = TimeSeriesTransformer(dim_val=dim_val, input_size=input_size, \n",
    "                              batch_first=True, dec_seq_len=dec_seq_len,\n",
    "                              out_seq_len=output_sequence_length, n_decoder_layers=n_decoder_layers,\n",
    "                              n_encoder_layers=n_encoder_layers, n_heads=n_heads).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e915834-a715-4bf2-8cef-3a4cf2ef9c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "# train_size = int(0.8 * len(mastar))\n",
    "# test_size = len(mastar) - train_size\n",
    "# train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# model = nn.Transformer(d_model=1, nhead=1, num_encoder_layers=12, batch_first=True).to(device)\n",
    "\n",
    "\n",
    "tr_loader = DataLoader(\n",
    "    mastar, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    # shuffle=True, num_workers=1, \n",
    "    )\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3323e60-9507-42cc-a062-c0cd718d71f9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #25  loss:0.8132 time:6.86 s\n",
      "Iteration #50  loss:0.2023 time:6.88 s\n",
      "Iteration #75  loss:0.0248 time:6.92 s\n",
      "Iteration #100  loss:0.3097 time:6.93 s\n",
      "Iteration #125  loss:0.4482 time:6.95 s\n",
      "Iteration #150  loss:0.0822 time:6.96 s\n",
      "Iteration #175  loss:0.5400 time:6.98 s\n",
      "Iteration #200  loss:0.4554 time:6.97 s\n",
      "Iteration #225  loss:0.7869 time:7.00 s\n",
      "Iteration #250  loss:0.0420 time:7.17 s\n",
      "Iteration #275  loss:0.1291 time:7.06 s\n",
      "Iteration #300  loss:0.1088 time:7.15 s\n",
      "Iteration #325  loss:0.0925 time:7.11 s\n",
      "Iteration #350  loss:3.0063 time:7.07 s\n",
      "Iteration #375  loss:0.4748 time:7.03 s\n",
      "Iteration #400  loss:0.8285 time:7.14 s\n",
      "Iteration #425  loss:0.3934 time:7.03 s\n",
      "Iteration #450  loss:1.1123 time:7.03 s\n",
      "Iteration #475  loss:0.3590 time:7.06 s\n",
      "Iteration #500  loss:0.6180 time:7.10 s\n",
      "Iteration #525  loss:0.3641 time:7.00 s\n",
      "Iteration #550  loss:0.4948 time:7.01 s\n",
      "Iteration #575  loss:0.1921 time:7.05 s\n",
      "Iteration #600  loss:0.1626 time:6.98 s\n",
      "Iteration #625  loss:0.1151 time:7.00 s\n",
      "Iteration #650  loss:0.7509 time:6.98 s\n",
      "Iteration #675  loss:0.2453 time:7.09 s\n",
      "Iteration #700  loss:0.2742 time:6.97 s\n",
      "Iteration #725  loss:0.3386 time:7.00 s\n",
      "Iteration #750  loss:0.1240 time:6.98 s\n",
      "Iteration #775  loss:0.4674 time:7.00 s\n",
      "Iteration #800  loss:0.0778 time:7.02 s\n",
      "Iteration #825  loss:0.2889 time:6.97 s\n",
      "Iteration #850  loss:0.3170 time:7.01 s\n",
      "Iteration #875  loss:0.0169 time:7.04 s\n",
      "Iteration #900  loss:0.0767 time:6.99 s\n",
      "Iteration #925  loss:0.1825 time:7.11 s\n",
      "Iteration #950  loss:0.1449 time:6.93 s\n",
      "Iteration #975  loss:0.3119 time:6.91 s\n",
      "Iteration #1000  loss:0.0433 time:7.12 s\n",
      "Iteration #1025  loss:0.0676 time:7.03 s\n",
      "Iteration #1050  loss:0.0450 time:7.03 s\n",
      "Iteration #1075  loss:0.0740 time:7.02 s\n",
      "Iteration #1100  loss:0.0908 time:7.01 s\n",
      "Iteration #1125  loss:0.2655 time:7.06 s\n",
      "Iteration #1150  loss:1.6040 time:7.05 s\n",
      "Iteration #1175  loss:0.0134 time:7.00 s\n",
      "Iteration #1200  loss:1.1372 time:7.00 s\n",
      "Iteration #1225  loss:0.1811 time:7.04 s\n",
      "Iteration #1250  loss:0.0580 time:7.05 s\n",
      "Iteration #1275  loss:0.2184 time:7.04 s\n",
      "Iteration #1300  loss:0.0505 time:7.10 s\n",
      "Iteration #1325  loss:0.1100 time:7.14 s\n",
      "Iteration #1350  loss:0.0034 time:7.02 s\n",
      "Iteration #1375  loss:0.0459 time:6.96 s\n",
      "Iteration #1400  loss:0.2445 time:7.00 s\n",
      "Iteration #1425  loss:0.0541 time:6.99 s\n",
      "Iteration #1450  loss:0.0594 time:7.04 s\n",
      "Iteration #1475  loss:0.0163 time:7.07 s\n",
      "Iteration #1500  loss:0.0343 time:7.02 s\n",
      "Iteration #1525  loss:0.0185 time:6.95 s\n",
      "Iteration #1550  loss:0.0128 time:6.95 s\n",
      "Iteration #1575  loss:0.0894 time:6.98 s\n",
      "Iteration #1600  loss:0.1682 time:7.09 s\n",
      "Iteration #1625  loss:0.0530 time:7.08 s\n",
      "Iteration #1650  loss:0.0062 time:7.39 s\n",
      "Iteration #1675  loss:0.0902 time:6.99 s\n",
      "Iteration #1700  loss:0.1800 time:7.02 s\n",
      "Iteration #1725  loss:0.1413 time:7.04 s\n",
      "Iteration #1750  loss:0.1531 time:7.06 s\n",
      "Iteration #1775  loss:0.2399 time:7.09 s\n",
      "Iteration #1800  loss:0.2242 time:7.02 s\n",
      "Iteration #1825  loss:3.3526 time:7.15 s\n",
      "Iteration #1850  loss:0.0074 time:7.06 s\n",
      "Iteration #1875  loss:0.0385 time:7.14 s\n",
      "Iteration #1900  loss:0.7441 time:7.15 s\n",
      "Iteration #1925  loss:0.0411 time:7.05 s\n",
      "Iteration #1950  loss:0.0374 time:7.49 s\n",
      "Iteration #1975  loss:0.0557 time:7.01 s\n",
      "Iteration #2000  loss:0.0083 time:6.97 s\n",
      "Iteration #2025  loss:0.1242 time:7.13 s\n",
      "Iteration #2050  loss:0.0417 time:7.07 s\n",
      "Iteration #2075  loss:0.0169 time:6.96 s\n",
      "Iteration #2100  loss:0.0129 time:7.05 s\n",
      "Iteration #2125  loss:0.1832 time:7.08 s\n",
      "Iteration #2150  loss:0.5581 time:6.94 s\n",
      "Iteration #2175  loss:0.0524 time:7.02 s\n",
      "Iteration #2200  loss:0.0826 time:6.97 s\n",
      "Iteration #2225  loss:0.0692 time:6.95 s\n",
      "Iteration #2250  loss:0.0365 time:6.98 s\n",
      "Iteration #2275  loss:0.0055 time:6.96 s\n",
      "Iteration #2300  loss:0.1067 time:7.04 s\n",
      "Iteration #2325  loss:0.0590 time:7.03 s\n",
      "Iteration #2350  loss:0.1184 time:7.03 s\n",
      "Iteration #2375  loss:0.0179 time:7.06 s\n",
      "Iteration #2400  loss:0.0452 time:7.01 s\n",
      "Iteration #2425  loss:0.0215 time:7.01 s\n",
      "Iteration #2450  loss:0.1248 time:6.96 s\n",
      "Iteration #2475  loss:0.0889 time:7.09 s\n",
      "Iteration #2500  loss:0.0152 time:6.90 s\n",
      "Iteration #2525  loss:0.1590 time:7.03 s\n",
      "Iteration #2550  loss:0.0260 time:7.08 s\n",
      "Iteration #2575  loss:0.0431 time:7.00 s\n",
      "Iteration #2600  loss:0.0084 time:7.24 s\n",
      "Iteration #2625  loss:0.0851 time:7.04 s\n",
      "Iteration #2650  loss:0.0575 time:7.05 s\n",
      "Iteration #2675  loss:0.0651 time:7.08 s\n",
      "Iteration #2700  loss:0.0063 time:7.05 s\n",
      "Iteration #2725  loss:0.0517 time:7.10 s\n",
      "Iteration #2750  loss:0.0344 time:6.99 s\n",
      "Iteration #2775  loss:0.0480 time:7.03 s\n",
      "Iteration #2800  loss:0.0491 time:7.02 s\n",
      "Iteration #2825  loss:0.0037 time:7.05 s\n",
      "Iteration #2850  loss:0.1553 time:6.91 s\n",
      "Iteration #2875  loss:0.0219 time:7.04 s\n",
      "Iteration #2900  loss:0.0264 time:6.97 s\n",
      "Iteration #2925  loss:0.0917 time:7.06 s\n",
      "Iteration #2950  loss:0.1731 time:7.04 s\n",
      "Iteration #2975  loss:0.1249 time:7.08 s\n",
      "Iteration #3000  loss:0.0955 time:6.93 s\n",
      "Iteration #3025  loss:0.0765 time:7.00 s\n",
      "Iteration #3050  loss:0.0272 time:7.07 s\n",
      "Iteration #3075  loss:0.0109 time:6.92 s\n",
      "Iteration #3100  loss:0.0282 time:6.94 s\n",
      "Iteration #3125  loss:0.0116 time:7.00 s\n",
      "Iteration #3150  loss:0.1639 time:7.07 s\n",
      "Iteration #3175  loss:0.2971 time:6.94 s\n",
      "Iteration #3200  loss:0.1196 time:7.06 s\n",
      "Iteration #3225  loss:0.0197 time:7.05 s\n",
      "Iteration #3250  loss:0.0973 time:7.12 s\n",
      "Iteration #3275  loss:0.0638 time:7.01 s\n",
      "Iteration #3300  loss:0.1374 time:7.02 s\n",
      "Iteration #3325  loss:0.0519 time:7.15 s\n",
      "Iteration #3350  loss:0.0555 time:7.12 s\n",
      "Iteration #3375  loss:0.0079 time:7.08 s\n",
      "Iteration #3400  loss:0.0271 time:7.13 s\n",
      "Iteration #3425  loss:0.1295 time:7.05 s\n",
      "Iteration #3450  loss:0.0110 time:6.98 s\n",
      "Iteration #3475  loss:0.0728 time:7.05 s\n",
      "Iteration #3500  loss:0.0575 time:6.91 s\n",
      "Iteration #3525  loss:0.0297 time:6.97 s\n",
      "Iteration #3550  loss:0.1583 time:6.93 s\n",
      "Iteration #3575  loss:0.1667 time:6.96 s\n",
      "Iteration #3600  loss:0.1100 time:6.97 s\n",
      "Iteration #3625  loss:0.0050 time:6.99 s\n",
      "Iteration #3650  loss:0.8765 time:6.96 s\n",
      "Iteration #3675  loss:0.0224 time:7.04 s\n",
      "Iteration #3700  loss:1.2857 time:7.10 s\n",
      "Iteration #3725  loss:0.0959 time:7.07 s\n",
      "Iteration #3750  loss:0.0571 time:7.04 s\n",
      "Iteration #3775  loss:0.0020 time:6.96 s\n",
      "Iteration #3800  loss:0.6025 time:7.06 s\n",
      "Iteration #3825  loss:0.2294 time:7.01 s\n",
      "Iteration #3850  loss:0.1526 time:6.98 s\n",
      "Iteration #3875  loss:0.0416 time:6.98 s\n",
      "Iteration #3900  loss:0.0305 time:7.10 s\n",
      "Iteration #3925  loss:0.0050 time:7.03 s\n",
      "Iteration #3950  loss:0.0378 time:7.01 s\n",
      "Iteration #3975  loss:0.0579 time:6.95 s\n",
      "Iteration #4000  loss:0.0670 time:7.01 s\n",
      "Iteration #4025  loss:0.0284 time:7.20 s\n",
      "Iteration #4050  loss:0.0172 time:7.08 s\n",
      "Iteration #4075  loss:0.0899 time:7.17 s\n",
      "Iteration #4100  loss:0.0439 time:7.13 s\n",
      "Iteration #4125  loss:0.0234 time:7.10 s\n",
      "Iteration #4150  loss:0.0061 time:6.99 s\n",
      "Iteration #4175  loss:0.0253 time:7.06 s\n",
      "Iteration #4200  loss:0.0058 time:7.13 s\n",
      "Iteration #4225  loss:0.0812 time:7.01 s\n",
      "Iteration #4250  loss:0.0861 time:7.11 s\n",
      "Iteration #4275  loss:0.0435 time:6.95 s\n",
      "Iteration #4300  loss:0.1089 time:7.03 s\n",
      "Iteration #4325  loss:0.9766 time:7.06 s\n",
      "Iteration #4350  loss:0.0229 time:7.01 s\n",
      "Iteration #4375  loss:0.3712 time:7.11 s\n",
      "Iteration #4400  loss:0.2995 time:7.08 s\n",
      "Iteration #4425  loss:0.1448 time:6.99 s\n",
      "Iteration #4450  loss:0.0221 time:7.00 s\n",
      "Iteration #4475  loss:0.2195 time:6.97 s\n",
      "Iteration #4500  loss:0.0606 time:7.06 s\n",
      "Iteration #4525  loss:0.0213 time:7.08 s\n",
      "Iteration #4550  loss:0.0112 time:7.06 s\n",
      "Iteration #4575  loss:0.1313 time:6.99 s\n",
      "Iteration #4600  loss:0.0095 time:7.04 s\n",
      "Iteration #4625  loss:0.0204 time:7.05 s\n",
      "Iteration #4650  loss:1.0009 time:7.02 s\n",
      "Iteration #4675  loss:0.0190 time:6.99 s\n",
      "Iteration #4700  loss:0.1711 time:7.05 s\n",
      "Iteration #4725  loss:0.0072 time:7.06 s\n",
      "Iteration #4750  loss:0.0365 time:7.01 s\n",
      "Iteration #4775  loss:0.0206 time:6.98 s\n",
      "Iteration #4800  loss:0.0932 time:7.03 s\n",
      "Iteration #4825  loss:0.0230 time:7.00 s\n",
      "Iteration #4850  loss:0.0899 time:7.02 s\n",
      "Iteration #4875  loss:0.0124 time:7.10 s\n",
      "Iteration #4900  loss:0.0224 time:7.14 s\n",
      "Iteration #4925  loss:0.0251 time:7.07 s\n",
      "Iteration #4950  loss:0.0806 time:6.99 s\n",
      "Iteration #4975  loss:0.0642 time:7.00 s\n",
      "Iteration #5000  loss:0.0170 time:6.94 s\n",
      "Iteration #5025  loss:0.0284 time:7.08 s\n",
      "Iteration #5050  loss:0.0180 time:7.03 s\n",
      "Iteration #5075  loss:0.0145 time:7.00 s\n",
      "Iteration #5100  loss:0.0064 time:6.98 s\n",
      "Iteration #5125  loss:0.1067 time:7.03 s\n",
      "Iteration #5150  loss:0.0484 time:7.07 s\n",
      "Iteration #5175  loss:0.0454 time:7.12 s\n",
      "Iteration #5200  loss:0.0377 time:7.08 s\n",
      "Iteration #5225  loss:0.0605 time:7.04 s\n",
      "Iteration #5250  loss:0.0187 time:6.97 s\n",
      "Iteration #5275  loss:0.0512 time:7.04 s\n",
      "Iteration #5300  loss:0.0147 time:6.94 s\n",
      "Iteration #5325  loss:0.0542 time:6.98 s\n",
      "Iteration #5350  loss:0.2010 time:7.12 s\n",
      "Iteration #5375  loss:0.0277 time:6.99 s\n",
      "Iteration #5400  loss:0.0952 time:7.13 s\n",
      "Iteration #5425  loss:0.0623 time:7.07 s\n",
      "Iteration #5450  loss:0.1541 time:6.91 s\n",
      "Iteration #5475  loss:0.1060 time:7.08 s\n",
      "Iteration #5500  loss:0.0567 time:7.03 s\n",
      "Iteration #5525  loss:0.0898 time:7.04 s\n",
      "Iteration #5550  loss:0.0144 time:6.99 s\n",
      "Iteration #5575  loss:0.0182 time:7.00 s\n",
      "Iteration #5600  loss:0.0126 time:7.02 s\n",
      "Iteration #5625  loss:0.0117 time:7.10 s\n",
      "Iteration #5650  loss:0.0247 time:7.09 s\n",
      "Iteration #5675  loss:0.1491 time:7.53 s\n",
      "Iteration #5700  loss:0.1395 time:7.08 s\n",
      "Iteration #5725  loss:0.0615 time:7.14 s\n",
      "Iteration #5750  loss:0.0870 time:7.08 s\n",
      "Iteration #5775  loss:0.1067 time:7.04 s\n",
      "Iteration #5800  loss:0.1045 time:7.15 s\n",
      "Iteration #5825  loss:0.0418 time:7.06 s\n",
      "Iteration #5850  loss:0.0617 time:7.01 s\n",
      "Iteration #5875  loss:0.0533 time:7.15 s\n",
      "Iteration #5900  loss:0.0461 time:7.07 s\n",
      "Iteration #5925  loss:0.1200 time:7.02 s\n",
      "Iteration #5950  loss:0.0129 time:7.05 s\n",
      "Iteration #5975  loss:0.5504 time:7.03 s\n",
      "Iteration #6000  loss:0.0282 time:6.97 s\n",
      "Iteration #6025  loss:0.0040 time:7.03 s\n",
      "Iteration #6050  loss:0.1857 time:7.06 s\n",
      "Iteration #6075  loss:0.1218 time:6.99 s\n",
      "Iteration #6100  loss:0.0865 time:7.01 s\n",
      "Iteration #6125  loss:0.0349 time:7.07 s\n",
      "Iteration #6150  loss:0.7958 time:7.04 s\n",
      "Iteration #6175  loss:0.0218 time:7.03 s\n",
      "Iteration #6200  loss:0.6255 time:6.95 s\n",
      "Iteration #6225  loss:0.0202 time:7.04 s\n",
      "Iteration #6250  loss:0.0936 time:7.00 s\n",
      "Iteration #6275  loss:0.0465 time:6.97 s\n",
      "Iteration #6300  loss:0.1347 time:7.04 s\n",
      "Iteration #6325  loss:0.2393 time:7.06 s\n",
      "Iteration #6350  loss:0.0858 time:7.01 s\n",
      "Iteration #6375  loss:0.0293 time:7.07 s\n",
      "Iteration #6400  loss:0.0137 time:6.97 s\n",
      "Iteration #6425  loss:0.2518 time:6.99 s\n",
      "Iteration #6450  loss:0.0468 time:7.00 s\n",
      "Iteration #6475  loss:0.0607 time:7.05 s\n",
      "Iteration #6500  loss:0.1942 time:7.05 s\n",
      "Iteration #6525  loss:0.0350 time:7.01 s\n",
      "Iteration #6550  loss:0.0301 time:7.05 s\n",
      "Iteration #6575  loss:0.0156 time:7.13 s\n",
      "Iteration #6600  loss:0.0727 time:7.13 s\n",
      "Iteration #6625  loss:0.0181 time:7.03 s\n",
      "Iteration #6650  loss:0.0554 time:7.06 s\n",
      "Iteration #6675  loss:0.0148 time:7.01 s\n",
      "Iteration #6700  loss:0.0352 time:7.04 s\n",
      "Iteration #6725  loss:0.0450 time:7.01 s\n",
      "Iteration #6750  loss:0.0153 time:6.96 s\n",
      "Iteration #6775  loss:0.1183 time:6.94 s\n",
      "Iteration #6800  loss:0.0139 time:7.05 s\n",
      "Iteration #6825  loss:1.2198 time:6.97 s\n",
      "Iteration #6850  loss:0.0098 time:7.10 s\n",
      "Iteration #6875  loss:0.0905 time:7.02 s\n",
      "Iteration #6900  loss:0.1483 time:7.00 s\n",
      "Iteration #6925  loss:0.0285 time:7.02 s\n",
      "Iteration #6950  loss:0.0174 time:6.98 s\n",
      "Iteration #6975  loss:0.0095 time:6.99 s\n",
      "Iteration #7000  loss:0.0253 time:7.00 s\n",
      "Iteration #7025  loss:0.0109 time:7.06 s\n",
      "Iteration #7050  loss:0.0175 time:7.01 s\n",
      "Iteration #7075  loss:0.0033 time:7.05 s\n",
      "Iteration #7100  loss:0.0068 time:7.03 s\n",
      "Iteration #7125  loss:0.1006 time:7.26 s\n",
      "Iteration #7150  loss:1.8972 time:6.96 s\n",
      "Iteration #7175  loss:0.0126 time:6.98 s\n",
      "Iteration #7200  loss:0.1009 time:7.11 s\n",
      "Iteration #7225  loss:0.0028 time:7.01 s\n",
      "Iteration #7250  loss:0.0205 time:7.07 s\n",
      "Iteration #7275  loss:0.2868 time:7.02 s\n",
      "Iteration #7300  loss:0.0212 time:7.10 s\n",
      "Iteration #7325  loss:0.0230 time:7.05 s\n",
      "Iteration #7350  loss:0.0400 time:6.94 s\n",
      "Iteration #7375  loss:0.0099 time:6.99 s\n",
      "Iteration #7400  loss:0.0040 time:7.07 s\n",
      "Iteration #7425  loss:0.0039 time:6.99 s\n",
      "Iteration #7450  loss:0.0150 time:7.01 s\n",
      "Iteration #7475  loss:0.0663 time:7.02 s\n",
      "Iteration #7500  loss:0.1425 time:7.02 s\n",
      "Iteration #7525  loss:0.0167 time:7.05 s\n",
      "Iteration #7550  loss:0.0173 time:7.08 s\n",
      "Iteration #7575  loss:0.0173 time:6.97 s\n",
      "Iteration #7600  loss:0.0097 time:7.10 s\n",
      "Iteration #7625  loss:0.0544 time:7.06 s\n",
      "Iteration #7650  loss:0.0448 time:6.96 s\n",
      "Iteration #7675  loss:0.0815 time:7.05 s\n",
      "Iteration #7700  loss:0.0197 time:7.11 s\n",
      "Iteration #7725  loss:0.0732 time:7.05 s\n",
      "Iteration #7750  loss:0.0033 time:7.05 s\n",
      "Iteration #7775  loss:0.0768 time:6.99 s\n",
      "Iteration #7800  loss:0.0349 time:6.98 s\n",
      "Iteration #7825  loss:0.2291 time:6.97 s\n",
      "Iteration #7850  loss:0.0487 time:6.96 s\n",
      "Iteration #7875  loss:0.0125 time:7.05 s\n",
      "Iteration #7900  loss:0.0450 time:6.98 s\n",
      "Iteration #7925  loss:0.0912 time:7.07 s\n",
      "Iteration #7950  loss:0.0177 time:7.04 s\n",
      "Iteration #7975  loss:0.0567 time:7.06 s\n",
      "Iteration #8000  loss:0.0669 time:7.03 s\n",
      "Iteration #8025  loss:0.1817 time:7.11 s\n",
      "Iteration #8050  loss:0.0067 time:6.99 s\n",
      "Iteration #8075  loss:0.0033 time:6.96 s\n",
      "Iteration #8100  loss:0.0523 time:7.09 s\n",
      "Iteration #8125  loss:0.0057 time:7.08 s\n",
      "Iteration #8150  loss:0.1202 time:7.04 s\n",
      "Iteration #8175  loss:0.1404 time:7.02 s\n",
      "Iteration #8200  loss:0.0019 time:7.01 s\n",
      "Iteration #8225  loss:0.0122 time:7.09 s\n",
      "Iteration #8250  loss:0.0790 time:7.14 s\n",
      "Iteration #8275  loss:0.0313 time:7.03 s\n",
      "Iteration #8300  loss:0.1646 time:6.98 s\n",
      "Iteration #8325  loss:0.0244 time:7.07 s\n",
      "Iteration #8350  loss:0.0069 time:7.00 s\n",
      "Iteration #8375  loss:0.0421 time:6.96 s\n",
      "Iteration #8400  loss:0.0783 time:6.94 s\n",
      "Iteration #8425  loss:0.2116 time:7.09 s\n",
      "Iteration #8450  loss:0.0360 time:7.04 s\n",
      "Iteration #8475  loss:0.0267 time:7.02 s\n",
      "Iteration #8500  loss:0.0209 time:7.05 s\n",
      "Iteration #8525  loss:0.0128 time:7.03 s\n",
      "Iteration #8550  loss:0.2859 time:7.06 s\n",
      "Iteration #8575  loss:0.0190 time:7.07 s\n",
      "Iteration #8600  loss:0.0065 time:7.07 s\n",
      "Iteration #8625  loss:0.0431 time:7.07 s\n",
      "Iteration #8650  loss:0.2598 time:7.04 s\n",
      "Iteration #8675  loss:0.0406 time:7.05 s\n",
      "Iteration #8700  loss:1.1725 time:7.02 s\n",
      "Iteration #8725  loss:0.0381 time:7.06 s\n",
      "Iteration #8750  loss:0.0030 time:7.04 s\n",
      "Iteration #8775  loss:0.0385 time:7.05 s\n",
      "Iteration #8800  loss:0.0232 time:7.01 s\n",
      "Iteration #8825  loss:0.2823 time:7.05 s\n",
      "Iteration #8850  loss:0.0129 time:7.04 s\n",
      "Iteration #8875  loss:0.1497 time:7.06 s\n",
      "Iteration #8900  loss:0.0182 time:6.99 s\n",
      "Iteration #8925  loss:0.0684 time:6.99 s\n",
      "Iteration #8950  loss:0.0058 time:7.00 s\n",
      "Iteration #8975  loss:0.0010 time:6.99 s\n",
      "Iteration #9000  loss:0.0379 time:6.96 s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     28\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m specs, pars, output\n",
      "File \u001b[0;32m/data/jdli/anaconda3/envs/gaia/lib/python3.9/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/jdli/anaconda3/envs/gaia/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/jdli/anaconda3/envs/gaia/lib/python3.9/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    155\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 157\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/data/jdli/anaconda3/envs/gaia/lib/python3.9/site-packages/torch/optim/adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 213\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/jdli/anaconda3/envs/gaia/lib/python3.9/site-packages/torch/optim/adam.py:263\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    262\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 263\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable:\n\u001b[1;32m    266\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_loss = 0.\n",
    "log_interval = 200\n",
    "num_epochs = 10\n",
    "\n",
    "itr = 1\n",
    "model.train()\n",
    "num_iters  = 25\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # src_mask = generate_square_subsequent_mask(BATCH_SIZE).to(device)\n",
    "    num_batches = len(mastar)//BATCH_SIZE\n",
    "    \n",
    "    for batch, (specs, pars) in enumerate(tr_loader):\n",
    "        start = time.time()\n",
    "        seq_len = specs.size(0)\n",
    "        \n",
    "#         if seq_len!=BATCH_SIZE:  # only on last batch\n",
    "#             src_mask = src_mask[:seq_len, :seq_len]\n",
    "            \n",
    "        # specs = list(s.to(device) for s in specs)\n",
    "        output = model(specs, pars)\n",
    "        # LOSS For torchvision.models.detection.retinanet_resnet50_fpn, RetinaNet\n",
    "        loss = criterion(output, pars)\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        del specs, pars, output\n",
    "        \n",
    "        if itr%num_iters == 0:\n",
    "            end = time.time()\n",
    "            print(f\"Iteration #%d  loss:%.4f time:%.2f s\"%(itr, loss_value, (end-start)*num_iters))\n",
    "                # writer.add_scalar('training loss = ',loss_value,epoch*itr)\n",
    "        itr+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70ae2881-b4e9-42e2-bd4b-bf0424479ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5904],\n",
       "         [ 3.6186]],\n",
       "\n",
       "        [[ 3.0032],\n",
       "         [10.8649]]], device='cuda:1', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3aaac453-2cc2-4ef2-9710-b1a9c2700a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6820],\n",
       "         [ 4.1115]],\n",
       "\n",
       "        [[ 3.0136],\n",
       "         [11.3146]]], device='cuda:1')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fdc4441-855d-409e-bc9b-4032998b18cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of TransformerEncoder(\n",
       "  (layers): ModuleList(\n",
       "    (0): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.2, inplace=False)\n",
       "      (dropout2): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (1): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.2, inplace=False)\n",
       "      (dropout2): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (2): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.2, inplace=False)\n",
       "      (dropout2): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (3): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.2, inplace=False)\n",
       "      (dropout2): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519ec084-6a0a-4ad0-b779-3d33416c90c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
