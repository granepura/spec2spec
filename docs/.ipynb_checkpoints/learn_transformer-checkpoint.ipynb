{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "943dcd57-1f27-4ffc-9cb4-a6f6985c7842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "import time\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac8838d5-44a9-4574-b6cf-13903472abaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/jdli/TransSpectra/\")\n",
    "\n",
    "from transformer import TransformerReg,generate_square_subsequent_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a6568350-894a-4352-b676-d46557c48084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 450 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from data import GaiaXPlabel_forcast\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_dir = \"/data/jdli/gaia/\"\n",
    "tr_file = \"ap17_wise_xpcont_cut.npy\"\n",
    "\n",
    "device = torch.device('cuda:1')\n",
    "TOTAL_NUM = 1000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "gdata  = GaiaXPlabel_forcast(data_dir+tr_file, total_num=TOTAL_NUM, part_train=True, device=device)\n",
    "\n",
    "val_size = int(0.1*len(gdata))\n",
    "A_size = int(0.5*(len(gdata)-val_size))\n",
    "B_size = len(gdata) - A_size - val_size\n",
    "\n",
    "A_dataset, B_dataset, val_dataset = torch.utils.data.random_split(gdata, [A_size, B_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "print(len(A_dataset), len(B_dataset), len(val_dataset))\n",
    "\n",
    "A_loader = DataLoader(A_dataset, batch_size=BATCH_SIZE)\n",
    "B_loader = DataLoader(B_dataset, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b2e3d664-a82c-4110-9bc1-23847eaf7fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerReg(\n",
    "    dim_val=64, input_size=1,\n",
    "    batch_first=True, \n",
    "    enc_seq_len=30, dec_seq_len=4,\n",
    "    out_seq_len=4, \n",
    "    n_decoder_layers=2,\n",
    "    n_encoder_layers=2, \n",
    "    n_heads=4,\n",
    "    max_seq_len=30,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2fbe762e-363a-4975-87ea-4333e5f9129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = A_dataset[0]['x'].view(-1,30,1)\n",
    "tgt = A_dataset[0]['tgt'].view(-1,4,1)\n",
    "\n",
    "# Make src mask for decoder with size:\n",
    "# [batch_size*n_heads, output_sequence_length, enc_seq_len]\n",
    "src_mask = generate_square_subsequent_mask(\n",
    "    dim1=4, dim2=30\n",
    "    ).to(device)\n",
    "\n",
    "# Make tgt mask for decoder with size:\n",
    "# [batch_size*n_heads, output_sequence_length, output_sequence_length]\n",
    "tgt_mask = generate_square_subsequent_mask( \n",
    "    dim1=4, dim2=4\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d1597246-0ee4-4a2d-be27-8d508a1e09ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 1])\n",
      "torch.Size([5, 1, 1])\n",
      "torch.Size([5, 2, 1])\n",
      "torch.Size([5, 3, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4758],\n",
       "         [ 0.1749],\n",
       "         [ 0.6493],\n",
       "         [ 0.1224]],\n",
       "\n",
       "        [[-0.0355],\n",
       "         [ 0.1663],\n",
       "         [ 0.0221],\n",
       "         [ 0.2551]],\n",
       "\n",
       "        [[ 0.0329],\n",
       "         [ 0.5259],\n",
       "         [ 0.5337],\n",
       "         [ 0.3079]],\n",
       "\n",
       "        [[-0.0866],\n",
       "         [ 0.2090],\n",
       "         [ 0.2769],\n",
       "         [ 0.4881]],\n",
       "\n",
       "        [[ 0.9540],\n",
       "         [ 0.4800],\n",
       "         [ 0.6724],\n",
       "         [ 0.2874]]], device='cuda:1', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def infer(model: nn.Module, src: torch.Tensor, forecast_window:int,\n",
    "          device,) -> torch.Tensor:\n",
    "    \n",
    "    target_seq_dim = 1\n",
    "    tgt = src[:,-1,0].view(-1, 1, 1) # [bs, 1, 1]\n",
    "    print(tgt.size())\n",
    "    # Iteratively concatenate tgt with the first element in the prediction\n",
    "    for _ in range(forecast_window-1):\n",
    "        \n",
    "        dim_a = tgt.shape[1] #1,2,3,.. n\n",
    "        dim_b = src.shape[1] #30\n",
    "        \n",
    "        src_mask = generate_square_subsequent_mask(dim1=dim_a, dim2=dim_b).to(device)\n",
    "        tgt_mask = generate_square_subsequent_mask(dim1=dim_a, dim2=dim_a).to(device)\n",
    "        \n",
    "        prediction = model(src, tgt, src_mask, tgt_mask)\n",
    "\n",
    "        # Obtain the predicted value at t+1 where t is the last step \n",
    "        # represented in tgt\n",
    "        last_predicted_value = prediction[:,-1,:].view(-1,1,1) #[bs, 1]\n",
    "        \n",
    "        # Reshape from [batch_size, 1] --> [1, batch_size, 1]\n",
    "        # last_predicted_value = last_predicted_value\n",
    "        # print(last_predicted_value.detach(), last_predicted_value.size())\n",
    "        print(tgt.size())\n",
    "        # Detach the predicted element from the graph and concatenate with \n",
    "        # tgt in dimension 1 or 0\n",
    "        tgt = torch.cat((tgt, last_predicted_value), dim=target_seq_dim)\n",
    "    \n",
    "    src_mask = generate_square_subsequent_mask(dim1=4, dim2=30).to(device)\n",
    "    tgt_mask = generate_square_subsequent_mask(dim1=4, dim2=4).to(device)\n",
    "    \n",
    "    # Make final prediction\n",
    "    return model(src, tgt, src_mask, tgt_mask)\n",
    "\n",
    "src = torch.rand(5, 30, 1).to(device)\n",
    "infer(model=model, src=src, forecast_window=4, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f14caf6-da8f-487c-9d94-1808e12b774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_mask = generate_square_subsequent_mask(\n",
    "dim1=4, dim2=30\n",
    ").to(device)\n",
    "\n",
    "tgt_mask = generate_square_subsequent_mask( \n",
    "    dim1=4, dim2=4\n",
    "    ).to(device)\n",
    "\n",
    "# cost = torch.nn.GaussianNLLLosss(full=True, reduction='mean')\n",
    "\n",
    "\n",
    "\n",
    "def train_epoch(tr_loader):\n",
    "    # model.train()\n",
    "    model.train()\n",
    "    # model_mohaom.train()\n",
    "    total_loss = 0.\n",
    "    start = time.time()\n",
    "    for batch, data in enumerate(tr_loader):\n",
    "\n",
    "        output = model(data['x'].view(-1,enc_seq_len,1), \n",
    "        data['y'].view(-1,out_seq_len,1), src_mask, tgt_mask)\n",
    "        loss = cost(output, data['y'].view(-1,out_seq_len,1))\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        total_loss+=loss_value\n",
    "        del data, output\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"Epoch #%d tr loss:%.4f time:%.2f s\"%(epoch, total_loss/(batch+1), (end-start)))\n",
    "\n",
    "\n",
    "def eval(val_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0\n",
    "        for bs, data in enumerate(val_loader):\n",
    "            # output = model(data['x'], data['tgt'])\n",
    "            output = infer(model=model, src=data['x'].view(-1,enc_seq_len,1), forecast_window=4, device=device)\n",
    "\n",
    "            loss = cost(output, data['y'].view(-1,out_seq_len,1))\n",
    "            total_val_loss+=loss.item()\n",
    "\n",
    "    print(\"val loss:%.4f\"%(total_val_loss/(bs+1)))\n",
    "\n",
    "    # if epoch%5==0:\n",
    "    #     eval(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f4c2ec28-1a10-4187-a120-148271ba64d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sin(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9dad00d9-f079-4c61-a279-3a78ca0318cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ASPCAP_error' from 'data' (/home/jdli/TransSpectra/data.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [119]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ASPCAP_error\n\u001b[1;32m      3\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/jdli/sdss/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m tr_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhogg19_spec_tr.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ASPCAP_error' from 'data' (/home/jdli/TransSpectra/data.py)"
     ]
    }
   ],
   "source": [
    "from data import ASPCAP_error\n",
    "\n",
    "data_dir = \"/data/jdli/sdss/\"\n",
    "tr_file = \"hogg19_spec_tr.npy\"\n",
    "\n",
    "device = torch.device('cuda:1')\n",
    "TOTAL_NUM = 6000\n",
    "BATCH_SIZE = 3\n",
    "\n",
    "aspcap  = ASPCAP_error(data_dir+tr_file, total_num=TOTAL_NUM,\n",
    "part_train=False, device=device)\n",
    "\n",
    "# aspcap_val = ASPCAP(data_dir+val_file, device=device)\n",
    "train_size = int(0.5*len(aspcap))\n",
    "val_size = len(aspcap) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(aspcap, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "print(len(train_dataset), len(val_dataset))\n",
    "\n",
    "# tr_loader  = DataLoader(train_dataset, batch_size=BATCH_SIZE, )\n",
    "# val_loader = DataLoader(val_dataset,  batch_size=BATCH_SIZE, )\n",
    "tr_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e02f0b9-e719-4215-9443-3bc624674c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
