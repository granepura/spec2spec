{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "028456fc-f32e-4abb-a79f-c823b8097e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/jdli/anaconda3/envs/gaia/lib/python3.9/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 1. is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/data/jdli/anaconda3/envs/gaia/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/data/jdli/anaconda3/envs/gaia/lib/python3.9/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 1. is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "efb13241-1a7e-40fc-b276-cb96aafe9979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape_loss(y_pred, target):\n",
    "    loss = 2 * (y_pred - target).abs() / (y_pred.abs() + target.abs() + 1e-8)\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "def gen_trg_mask(length, device):\n",
    "    mask = torch.tril(torch.ones(length, length, device=device)) == 1\n",
    "\n",
    "    mask = (\n",
    "        mask.float()\n",
    "        .masked_fill(mask==0, float(\"-inf\"))\n",
    "        .masked_fill(mask==1, float(0.0))\n",
    "    )\n",
    "    return mask\n",
    "\n",
    "\n",
    "class Spec2HRd(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_encoder_inputs,\n",
    "        n_decoder_inputs,\n",
    "        n_outputs,\n",
    "        channels=512,\n",
    "        dropout=0.2,\n",
    "        lr=1e-4,\n",
    "        nhead=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_encoder_inputs = n_encoder_inputs\n",
    "        self.n_decoder_inputs = n_decoder_inputs\n",
    "        self.save_hyperparameters()\n",
    "        self.channels = channels\n",
    "        self.n_outputs = n_outputs\n",
    "        self.lr = lr\n",
    "        self.dropout = dropout\n",
    "\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=channels,\n",
    "            nhead=nhead,\n",
    "            dropout=self.dropout,\n",
    "            dim_feedforward=4*channels,\n",
    "        )\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=channels,\n",
    "            nhead=nhead,\n",
    "            dropout=self.dropout,\n",
    "            dim_feedforward=4 * channels,\n",
    "        )\n",
    "\n",
    "        self.encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=8)\n",
    "        self.decoder = torch.nn.TransformerDecoder(decoder_layer, num_layers=8)\n",
    "\n",
    "        self.input_projection = Linear(n_encoder_inputs, channels)\n",
    "        self.output_projection = Linear(n_decoder_inputs, channels)\n",
    "        self.input_pos_embedding = torch.nn.Embedding(1024, embedding_dim=channels)\n",
    "        self.target_pos_embedding = torch.nn.Embedding(1024, embedding_dim=channels)\n",
    "\n",
    "        # self.linear = Linear(channels, 2)\n",
    "        self.fc1 = Linear(channels, 64)\n",
    "        self.fc2 = Linear(64, n_outputs)\n",
    "        self.do = nn.Dropout(p=self.dropout)\n",
    "\n",
    "    def encode_src(self, src):\n",
    "        src_start = self.input_projection(src).permute(1, 0, 2)\n",
    "\n",
    "        in_sequence_len, batch_size = src_start.size(0), src_start.size(1)\n",
    "        pos_encoder = (\n",
    "            torch.arange(0, in_sequence_len, device=src.device)\n",
    "            .unsqueeze(0)\n",
    "            .repeat(batch_size, 1)\n",
    "        )\n",
    "        \n",
    "        pos_encoder = self.input_pos_embedding(pos_encoder).permute(1, 0, 2)\n",
    "\n",
    "        src = src_start + pos_encoder\n",
    "        src = self.encoder(src) + src_start\n",
    "\n",
    "        return src\n",
    "    \n",
    "    def decode_trg(self, trg, memory):\n",
    "\n",
    "        trg_start = self.output_projection(trg).permute(1, 0, 2)\n",
    "\n",
    "        out_sequence_len, batch_size = trg_start.size(1), trg_start.size(0)\n",
    "\n",
    "        pos_decoder = (\n",
    "            torch.arange(0, out_sequence_len, device=trg.device)\n",
    "            .unsqueeze(0)\n",
    "            .repeat(batch_size, 1)\n",
    "        )\n",
    "        pos_decoder = self.target_pos_embedding(pos_decoder).permute(1, 0, 2)\n",
    "\n",
    "        trg = pos_decoder + trg_start\n",
    "        \n",
    "        trg_mask = gen_trg_mask(out_sequence_len, trg.device)\n",
    "        out = self.decoder(tgt=trg, memory=memory, tgt_mask=trg_mask) + trg_start\n",
    "        out = out.permute(1, 0, 2)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        src = x\n",
    "        \n",
    "        enc_ouput = self.encode_src(src) # (1, bs, 512)\n",
    "        src = F.relu(enc_ouput) # (1, bs, 512)\n",
    "        \n",
    "        src = src.permute(1, 0, 2) #(bs, 1, 512)\n",
    "        src = src.view(-1, self.channels) # (bs, 512)\n",
    "\n",
    "        src = self.fc1(src) # (bs, 64)\n",
    "        src = F.relu(src)\n",
    "        \n",
    "        tgt_a = self.fc2(src) # (bs, 2)\n",
    "\n",
    "        dec_input = torch.concat((x.view(-1, self.n_encoder_inputs), tgt_a), dim=1).view(-1, 1, self.n_decoder_inputs)\n",
    "        \n",
    "        \n",
    "        out = self.decode_trg(trg=dec_input, \n",
    "                              memory=enc_ouput)# (1, bs, 512)\n",
    "        out = F.relu(out)\n",
    "        out = out.permute(1, 0, 2) #(bs, 1, 512)\n",
    "        out = out.view(-1, self.channels) # (bs, 512)\n",
    "        \n",
    "        out = self.fc1(out) # (bs, 64)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        out = self.fc2(out) # (bs, 2)\n",
    "        print(tgt_a.shape, out.shape)\n",
    "        return torch.concat((tgt_a, out), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a366f1de-2dbe-4d81-9535-96fdc2427ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2]) torch.Size([25, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 5 but got size 25 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [64]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# source = torch.rand(size=(32, 16, 9))\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# target_in = torch.rand(size=(32, 16, 8))\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# target_out = torch.rand(size=(32, 16, 1))\u001b[39;00m\n\u001b[1;32m      8\u001b[0m ts \u001b[38;5;241m=\u001b[39m Spec2HRd(n_encoder_inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m343\u001b[39m, n_decoder_inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m343\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m, n_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m345\u001b[39m, nhead\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/jdli/anaconda3/envs/gaia/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [63]\u001b[0m, in \u001b[0;36mSpec2HRd.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    129\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(out) \u001b[38;5;66;03m# (bs, 2)\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mprint\u001b[39m(tgt_a\u001b[38;5;241m.\u001b[39mshape, out\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 5 but got size 25 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "\n",
    "source = torch.rand(size=(5, 1, 343))\n",
    "target_in = torch.rand(size=(5, 1, 343))\n",
    "target_out = torch.rand(size=(5, 1, 4))\n",
    "\n",
    "# source = torch.rand(size=(32, 16, 9))\n",
    "# target_in = torch.rand(size=(32, 16, 8))\n",
    "# target_out = torch.rand(size=(32, 16, 1))\n",
    "ts = Spec2HRd(n_encoder_inputs=343, n_decoder_inputs=343+2, n_outputs=2, channels=345, nhead=3)\n",
    "pred = ts((source))\n",
    "\n",
    "# print(pred.size())\n",
    "\n",
    "# ts.training_step((source, target_in, target_out), batch_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7382c873-6bea-4f8d-996e-1d1ace89fd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 450 100\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/jdli/TransSpectra/\")\n",
    "from data import GaiaXPlabel_v2\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "data_dir = \"/data/jdli/gaia/\"\n",
    "tr_file = \"ap17_xp.npy\"\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "gdata  = GaiaXPlabel_v2(data_dir+tr_file, total_num=1000, part_train=True, device=device)\n",
    "\n",
    "val_size = int(0.1*len(gdata))\n",
    "A_size = int(0.5*(len(gdata)-val_size))\n",
    "B_size = len(gdata) - A_size - val_size\n",
    "\n",
    "A_dataset, B_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    gdata, [A_size, B_size, val_size], \n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "print(len(A_dataset), len(B_dataset), len(val_dataset))\n",
    "\n",
    "A_loader = DataLoader(A_dataset, batch_size=BATCH_SIZE)\n",
    "B_loader = DataLoader(B_dataset, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc77bfb-4a3d-47d7-83e3-afa692441cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
