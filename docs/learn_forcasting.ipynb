{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "028456fc-f32e-4abb-a79f-c823b8097e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/jdli/anaconda3/envs/gaia/lib/python3.9/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 1. is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/data/jdli/anaconda3/envs/gaia/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/data/jdli/anaconda3/envs/gaia/lib/python3.9/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 1. is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "efb13241-1a7e-40fc-b276-cb96aafe9979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape_loss(y_pred, target):\n",
    "    loss = 2 * (y_pred - target).abs() / (y_pred.abs() + target.abs() + 1e-8)\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "def gen_trg_mask(length, device):\n",
    "    mask = torch.tril(torch.ones(length, length, device=device)) == 1\n",
    "\n",
    "    mask = (\n",
    "        mask.float()\n",
    "        .masked_fill(mask==0, float(\"-inf\"))\n",
    "        .masked_fill(mask==1, float(0.0))\n",
    "    )\n",
    "    return mask\n",
    "\n",
    "\n",
    "class Spec2HRd(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_encoder_inputs,\n",
    "        n_decoder_inputs,\n",
    "        n_outputs,\n",
    "        channels=512,\n",
    "        dropout=0.2,\n",
    "        lr=1e-4,\n",
    "        nhead=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_encoder_inputs = n_encoder_inputs\n",
    "        self.n_decoder_inputs = n_decoder_inputs\n",
    "        self.save_hyperparameters()\n",
    "        self.channels = channels\n",
    "        self.n_outputs = n_outputs\n",
    "        self.lr = lr\n",
    "        self.dropout = dropout\n",
    "\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=channels,\n",
    "            nhead=nhead,\n",
    "            dropout=self.dropout,\n",
    "            dim_feedforward=4*channels,\n",
    "        )\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=channels,\n",
    "            nhead=nhead,\n",
    "            dropout=self.dropout,\n",
    "            dim_feedforward=4 * channels,\n",
    "        )\n",
    "\n",
    "        self.encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=8)\n",
    "        self.decoder = torch.nn.TransformerDecoder(decoder_layer, num_layers=8)\n",
    "\n",
    "        self.input_projection = Linear(n_encoder_inputs, channels)\n",
    "        self.output_projection = Linear(n_decoder_inputs, channels)\n",
    "        self.input_pos_embedding = torch.nn.Embedding(1024, embedding_dim=channels)\n",
    "        self.target_pos_embedding = torch.nn.Embedding(1024, embedding_dim=channels)\n",
    "\n",
    "        # self.linear = Linear(channels, 2)\n",
    "        self.fc1 = Linear(channels, 64)\n",
    "        self.fc2 = Linear(64, n_outputs)\n",
    "        self.do = nn.Dropout(p=self.dropout)\n",
    "\n",
    "    def encode_src(self, src):\n",
    "        src_start = self.input_projection(src).permute(1, 0, 2)\n",
    "\n",
    "        in_sequence_len, batch_size = src_start.size(0), src_start.size(1)\n",
    "        pos_encoder = (\n",
    "            torch.arange(0, in_sequence_len, device=src.device)\n",
    "            .unsqueeze(0)\n",
    "            .repeat(batch_size, 1)\n",
    "        )\n",
    "        \n",
    "        pos_encoder = self.input_pos_embedding(pos_encoder).permute(1, 0, 2)\n",
    "\n",
    "        src = src_start + pos_encoder\n",
    "        src = self.encoder(src) + src_start\n",
    "\n",
    "        return src\n",
    "    \n",
    "    def decode_trg(self, trg, memory):\n",
    "\n",
    "        trg_start = self.output_projection(trg).permute(1, 0, 2)\n",
    "\n",
    "        out_sequence_len, batch_size = trg_start.size(0), trg_start.size(1)\n",
    "\n",
    "        pos_decoder = (\n",
    "            torch.arange(0, out_sequence_len, device=trg.device)\n",
    "            .unsqueeze(0)\n",
    "            .repeat(batch_size, 1)\n",
    "        )\n",
    "        pos_decoder = self.target_pos_embedding(pos_decoder).permute(1, 0, 2)\n",
    "\n",
    "        trg = pos_decoder + trg_start\n",
    "        \n",
    "        trg_mask = gen_trg_mask(out_sequence_len, trg.device)\n",
    "        out = self.decoder(tgt=trg, memory=memory, tgt_mask=trg_mask) + trg_start\n",
    "        out = out.permute(1, 0, 2)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        src = x\n",
    "        \n",
    "        enc_ouput = self.encode_src(src) # (1, bs, 512)\n",
    "        src = F.relu(enc_ouput) # (1, bs, 512)\n",
    "        \n",
    "        src = src.permute(1, 0, 2) #(bs, 1, 512)\n",
    "        src = src.view(-1, self.channels) # (bs, 512)\n",
    "\n",
    "        src = self.fc1(src) # (bs, 64)\n",
    "        src = F.relu(src)\n",
    "        \n",
    "        tgt_a = self.fc2(src) # (bs, 2)\n",
    "\n",
    "        dec_input = torch.concat((x.view(-1, self.n_encoder_inputs), tgt_a), dim=1).view(-1, 1, self.n_decoder_inputs)\n",
    "        \n",
    "        \n",
    "        out = self.decode_trg(trg=dec_input, \n",
    "                              memory=enc_ouput)# (1, bs, 512)\n",
    "        out = F.relu(out)\n",
    "        out = out.permute(1, 0, 2) #(bs, 1, 512)\n",
    "        out = out.view(-1, self.channels) # (bs, 512)\n",
    "        \n",
    "        out = self.fc1(out) # (bs, 64)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        out = self.fc2(out) # (bs, 2)\n",
    "        return torch.concat((tgt_a, out), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a366f1de-2dbe-4d81-9535-96fdc2427ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "source = torch.rand(size=(5, 1, 343))\n",
    "target_in = torch.rand(size=(5, 1, 343))\n",
    "target_out = torch.rand(size=(5, 1, 4))\n",
    "\n",
    "# source = torch.rand(size=(32, 16, 9))\n",
    "# target_in = torch.rand(size=(32, 16, 8))\n",
    "# target_out = torch.rand(size=(32, 16, 1))\n",
    "ts = Spec2HRd(n_encoder_inputs=343, n_decoder_inputs=343+2, n_outputs=2, channels=345, nhead=3)\n",
    "pred = ts((source))\n",
    "\n",
    "# print(pred.size())\n",
    "\n",
    "# ts.training_step((source, target_in, target_out), batch_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4dbe7582-40fa-4fe7-9ffc-e15fb5098b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3433,  0.0634, -0.0731, -0.4559],\n",
       "        [ 0.2787,  0.0101, -0.0420, -0.2159],\n",
       "        [ 0.1437,  0.0069, -0.0386, -0.1618],\n",
       "        [ 0.0988, -0.0679,  0.2679, -0.1849],\n",
       "        [ 0.3113,  0.0133,  0.1352, -0.0711]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7382c873-6bea-4f8d-996e-1d1ace89fd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450 450 100\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/jdli/TransSpectra/\")\n",
    "from data import GaiaXPlabel_v2\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "data_dir = \"/data/jdli/gaia/\"\n",
    "tr_file = \"ap17_xp.npy\"\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "gdata  = GaiaXPlabel_v2(data_dir+tr_file, total_num=1000, part_train=True, device=device)\n",
    "\n",
    "val_size = int(0.1*len(gdata))\n",
    "A_size = int(0.5*(len(gdata)-val_size))\n",
    "B_size = len(gdata) - A_size - val_size\n",
    "\n",
    "A_dataset, B_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    gdata, [A_size, B_size, val_size], \n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "print(len(A_dataset), len(B_dataset), len(val_dataset))\n",
    "\n",
    "A_loader = DataLoader(A_dataset, batch_size=BATCH_SIZE)\n",
    "B_loader = DataLoader(B_dataset, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc77bfb-4a3d-47d7-83e3-afa692441cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
